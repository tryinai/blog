
# 学习 还是 搜索？从理解大数定律以及简单任务开始。

#### 一直坚信：规律一定是简单和容易理解的，只是很多人并不去探究源头以及过程，容易陷入表面概念的陷阱，导致难以理解。
#### 学习是根据经验改进的过程，哪些经验，如何改进？实际上，学习的过程，就是一个搜索的过程，利用可能的信息搜索目标的过程，往往需要考虑exploit-explore之间的均衡。


概率论是一种很有用的理论，可以帮助分析很多问题。显然，“分析”就是特别重要的。其实数学里最重要的就是分析和构建模式。

我们将待分析的对象看作是cover，比如图像，组成cover的基本元素看作是element。显然，cover就是一个包容element的容器，而element就是容器中的小球。

实际上，element是可以再分的，也可以重新组合成新的element'。

我们以灰度图像为例，来说明大数定律，以及中心极限定律。

对于灰度图像空间，cover就是每一个图像实例，element就是图像的像素pixel。当然，我们可以用频域空间变换（比如离散余弦变换等）得到对应频域的系数实例，在精度上会有些误差，我们仍然将这个
系数实例看作是cover，每个系数仍然看作是像素pixel。

也就是，cover = { e / e 属于 {0，1}^8 }, e 就是pixel或者小球。

一、我们可以将这些小球看作是同一类，或者说对于cover这个“黑箱”，我们所能知道的知识只有小球的取值的范围，丢失了小球的其他的信息。

那么给定任意一个cover，我们能够获得的信息就是cover中小球的取值的计数（直方图h）。

对于很多简单的图像分类任务而言，实际上知道这个直方图，就已经能够很好的进行分类了。比如对于只包含两种类型的图像，一种是直方图不存在明显差异的区域，一种是直方图存在明显差异的两个区域，显然，分类这些图像，
只需要根据直方图，寻找到一个反映直方图差异的合适的阈值，即可完美的执行分类任务。那么如何寻找到合适的阈值呢？

显然，最简单的办法，就是直接穷举搜索，直到找到合适的阈值为止。此时，我们需要搜集大量的图像，然后检查每一个阈值，看是否满足，显然计算量是很大的。

那么怎样做到更好呢？显然，就是要减少搜索空间的大小，也就是需要利用一些先验的知识/或者伪知识（假设）。

一种考虑就是，如果我们不检查每一张图像，而是检查描述这些图像本质特征的表示空间，那么就能大大的减少计算量。因为对于这个示例任务而言，我们关心的只有直方图h。
但此时，仍然是一个大空间。

另一种考虑就是，如果我们不检查真实的表示空间实例，而是检查一个描述这个真实空间实例的模型，那么我们就能直接利用模型的内在关联，做出合理的优化。

独立同分布模型（I.I.D）就是被大量利用的模型，尽管往往只是一种近似的模型，实际中表现得很好，因为我们实际关心的就是近似的解。将每个cover看作是一个随机变量，此时就是直方图h。假设直方图h
的每一种实例取值，都是采样自同一个概率空间。可以将这个过程，看作是从容器中取出小球，并且是有放回的取，这样每次取小球不影响容器中小球的分布。

那么，当我们取N次小球，不同取值的小球出现的次数C同N的比率r = C / N，会是怎样的一个情况呢？

大数定律告诉我们，r将以概率收敛于小球的在概率空间中的概率值p。为什么？

实际上，如果我们每次取小球都是按照未知的概率p在取（概率空间的假设），取N次小球显然不同取值的小球出现的次数就是Np，这是一个简单的数学计算。

我们应该明白的是，每次取小球实际上是有偏差的，这才是最关键的地方，重点在于“偏差”。假设，每次的偏差是一个不确定的值，但总是取值于某一个范围，可以看作是每次取小球的操作总是附带一个随机的扰动noise。当然，这个随机扰动noise可以为0，即每次都严格按照理想的未知概率p进行采样。但实际上，总是存在误差，比如环境噪声，比如计算代价，比如采样不够随机等等，导致总是存在扰动noise。针对这些大量的因子导致误差的分布，实际上正是高斯正态分布所反映的内容，也就是期望值为0，即大量的扰动noise的整体期望的结果为0.

这样不管是理想的取小球，还是针对大量影响因子的随机扰动的取小球，最终r都是以概率收敛于p，也就是r的期望值就是p。

而中心极限定律，告诉我们，随机采样不同的样本集，那么这些样本集的均值，服从高斯分布，实际上，这从上面的有关偏差的大量影响因子的随机扰动就可以直接得到。

因此，我们利用直方图h的独立同分布假设模型，只需要采样少量的样例，我们便能够得到有关直方图h的原始空间的合理的近似。这样我们就可以根据这个近似的空间，以少量的计算来搜索得到更加合理的阈值。

具体如何搜索呢？显然，对于这个示例的任务而言，只有两类，即输出为0或者1，并且只有一个参数即阈值（w,b），显然，利用简单的线性+sigmoid，即可表示，ph = sigmoid(hw+b)， 这个ph应该应该符合根据采样近似的空间的分布，即距离最小，或者最相似，或者交叉熵最小。实际上，这个搜索过程，就变成了调整参数(w,b)以最小化距离，根据大数定律，显然依概率收敛于真实的有关示例任务两个分类的概率分布。此时，我们就说阈值(w,b)完成了分类的任务。

然而，仔细想想，并不能保证，每个实例都能完美分类，总是存在误分类的可能。因为，我们是最小化概率分布的期望差异，我们只是尽可能的在期望上做到最好，实际上并没有有关具体每个cover的知识，类似我们关注的实际上是“法人”cover，而不是每个“真实的cover。

实际上，机器学习/深度学习的思路就是这样，深度学习通过构建更好的表示空间和更大的参数空间，来得到更好的近似。

## 

二、我们可以将这些小球看作是不同的类型，这个更复杂，以后有机会再写一点吧。

## 

好了，以上是简单的分析，具体可以参见经典的书籍和论文。也可能有一些错误，暂时也不想改正。



## 参考书籍：
- GoodFellow.《Deep Learning》.
- Daphne Koller. 《概率图模型》.



- ####  [回主页](./README.md) 

